{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using India server backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'আমি হাতি ভালবাসি!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import translators as ts\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "ps = PorterStemmer()\n",
    "ts.google(\"I love Elephants!\", \"en\", \"bn\")\n",
    "langs = list(ts._google.language_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = 0\n",
    "count = {}\n",
    "for i in tqdm(range(2914)):\n",
    "    with open(f\"../Object_casedocs/C{i+1}.txt\") as f:\n",
    "        text = f.read().split(\"\\n\")\n",
    "    m = max([len(t) for t in text])\n",
    "    if m>ans:\n",
    "        ans = m\n",
    "    if m>5000:\n",
    "        count[i+1] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(string,\n",
    "            tokenizer = nltk.RegexpTokenizer(r\"\\w+\"),\n",
    "            ps = PorterStemmer(),\n",
    "            stopwords = stopwords.words('english')):\n",
    "    '''\n",
    "    - A function to process a string and return a list of tokens.\n",
    "    - We tokenize the string, remove stopwords and numbers, and\n",
    "        finally stem the tokens to keep them in a list.\n",
    "    - This function will be used in all cases uniformly so that \n",
    "        we can compare \"APPLES WITH APPLES\".\n",
    "    '''\n",
    "    string = tokenizer.tokenize(string.lower()) # tokenize\n",
    "    tokens = [ps.stem(fl) for fl in string # stem tokens\n",
    "                if not fl.isnumeric() and # remove numbers\n",
    "                fl not in stopwords] # remove stopwords\n",
    "    return tokens # takes string as input and returns a list\n",
    "\n",
    "def augment(string, to, threshold = 5000):\n",
    "    string = string.split(\"\\n\")\n",
    "    text = []\n",
    "\n",
    "    for line in string:\n",
    "        if len(line)>threshold:\n",
    "            text += line.split(\".\")\n",
    "        elif line != \"\":\n",
    "            text.append(line)\n",
    "\n",
    "    ans = []\n",
    "    for tex in tqdm(text):\n",
    "        oth = ts.google(tex, \"en\", to)\n",
    "        eng = ts.google(oth, to, \"en\")\n",
    "        ans.append(eng)\n",
    "\n",
    "    return process(\". \".join(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Object_casedocs/C1.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = augment(text, \"bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.google(text, \"en\", \"bn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"../refining_seriously/\"\n",
    "def namestr(obj, namespace = globals()):\n",
    "    return [name for name in namespace if namespace[name] is obj][0]\n",
    "\n",
    "def print_json(query, n = 3, m = 5, k=6):\n",
    "    n = 3\n",
    "    print(f\"{namestr(query)} = \"+\"{\\n\", end=\"\")  # start of the json\n",
    "    l = sorted(list(query.keys()),\n",
    "            key=lambda x: int(x[k:]))\n",
    "    for QID in l[:n]:\n",
    "        print('\\t\"'+QID+'\":', query[QID][:m], \"\\b\\b, ......],\")\n",
    "    for i in range(2):\n",
    "        print(\"\\t...\")\n",
    "    for QID in l[-n:]:\n",
    "        print('\\t\"'+QID+'\":', query[QID][:m], \"\\b\\b, ......],\")\n",
    "    print(\"}\")  # end of the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior_cases = {\n",
      "\t\"C1\": ['masud', 'khan', 'v', 'state', 'uttar', ......],\n",
      "\t\"C2\": ['indian', 'oil', 'corpor', 'v', 'nepc', ......],\n",
      "\t\"C3\": ['gurpal', 'singh', 'v', 'state', 'punjab', ......],\n",
      "\t...\n",
      "\t...\n",
      "\t\"C2912\": ['dharangadhara', 'chemic', 'work', 'limit', 'v', ......],\n",
      "\t\"C2913\": ['central', 'bank', 'india', 'v', 'sethumadhavan', ......],\n",
      "\t\"C2914\": ['som', 'raj', 'soma', 'v', 'state', ......],\n",
      "}\n",
      "query = {\n",
      "\t\"AILA_Q1\": ['appel', 'februari', 'appoint', 'offic', 'grade', ......],\n",
      "\t\"AILA_Q2\": ['appel', 'us', 'examin', 'prime', 'wit', ......],\n",
      "\t\"AILA_Q3\": ['appeal', 'aris', 'judgment', 'learn', 'singl', ......],\n",
      "\t...\n",
      "\t...\n",
      "\t\"AILA_Q48\": ['whether', 'sanction', 'requir', 'initi', 'crimin', ......],\n",
      "\t\"AILA_Q49\": ['appel', 'patwari', 'work', 'villag', 'v1', ......],\n",
      "\t\"AILA_Q50\": ['peculiar', 'featur', 'appeal', 'special', 'leav', ......],\n",
      "}\n",
      "answers = {\n",
      "\t\"AILA_Q1\": ['C14', ......],\n",
      "\t\"AILA_Q2\": ['C27', ......],\n",
      "\t\"AILA_Q3\": ['C1', ......],\n",
      "\t...\n",
      "\t...\n",
      "\t\"AILA_Q48\": ['C82', ......],\n",
      "\t\"AILA_Q49\": ['C174', ......],\n",
      "\t\"AILA_Q50\": ['C27', ......],\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(loc+\"cases.json\") as f:\n",
    "    prior_cases = json.load(f)\n",
    "with open(loc+\"Query_doc.json\") as f:\n",
    "    query = json.load(f)\n",
    "with open(loc+\"answers.json\") as f:\n",
    "    answers = json.load(f)\n",
    "\n",
    "print_json(prior_cases, k=1)\n",
    "print_json(query)\n",
    "print_json(answers, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Prior Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = list(ts._google.language_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(langs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95b59b4b0e72d3e94105c3ab4f1a1e6e746e4c2a7c235241251baf92fb36381f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
