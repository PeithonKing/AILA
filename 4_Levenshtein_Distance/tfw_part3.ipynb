{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Copy Pasting the Metrices code\n",
    "def rank(a, val):\n",
    "    try: return sorted(a, reverse=True).index(val)+1\n",
    "    except ValueError: return None\n",
    "\n",
    "def average_precision(result, relevant):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    result = [0.7, 0.9, 0.8, 1.0, 0.2, 0.81]\n",
    "              C1  [C2]  [C3]  C4  [C5]  C6       # [x] just means x is relevant doc\n",
    "                    3     1         2\n",
    "\n",
    "    relevant = [\"C3\", \"C5\", \"C2\"]\n",
    "    \"\"\"\n",
    "    # print(result[:5], \"...\")\n",
    "    # print(relevant,)\n",
    "    AP = 0\n",
    "    for GivenRankM1, DocName in enumerate(relevant):\n",
    "        DocNum = int(DocName[1:])\n",
    "        # PredRank = rank(result, result[DocNum-1])\n",
    "        PredRank = sorted(result, reverse=True).index(result[DocNum-1])+1\n",
    "        AP += (GivenRankM1+1)/PredRank\n",
    "        # print(f\"{GivenRankM1+1}/{PredRank} = {(GivenRankM1+1)/PredRank}\")\n",
    "    # print(f\"getting AP: {AP} / {len(relevant)} = {AP/len(relevant)}\\n\")\n",
    "    return AP/len(relevant)\n",
    "\n",
    "def MAP(scores, answers):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "    scores = [[0.7, 0.9, 0.8, 1.0, 0.2, 0.81],\n",
    "              [0.47, 0.49, 0.48, 1.40, 0.42, 0.84],\n",
    "              ...\n",
    "              ...\n",
    "              ... 50 such lines]\n",
    "    answers = {\n",
    "    \t\"AILA_Q1\": ['C14', ......],\n",
    "    \t\"AILA_Q2\": ['C27', ......],\n",
    "    \t\"AILA_Q3\": ['C1', ......],\n",
    "    \t...\n",
    "    \t...\n",
    "    \t\"AILA_Q48\": ['C82', ......],\n",
    "    \t\"AILA_Q49\": ['C174', ......],\n",
    "    \t\"AILA_Q50\": ['C27', ......],\n",
    "    }\n",
    "    \"\"\"\n",
    "    n = len(scores)\n",
    "    ans = 0\n",
    "    for i in range(n):\n",
    "        ans += average_precision(scores[i], answers[f\"AILA_Q{i+1}\"])\n",
    "    return ans/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"fuzzy_ratio.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"../refining_seriously/\"\n",
    "with open(loc+\"answers.json\") as f:\n",
    "    answers = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.285243135381555"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP(data.iloc[:, 1:].to_numpy(), answers)*100"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95b59b4b0e72d3e94105c3ab4f1a1e6e746e4c2a7c235241251baf92fb36381f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
